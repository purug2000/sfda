{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.data.processors.utils import InputExample, InputFeatures\n",
    "from transformers.data.processors.glue import glue_convert_examples_to_features\n",
    "from transformers.data.processors.utils import DataProcessor\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sfda.models import sfdaRobertaNegation\n",
    "from sfda.train_utils import sfdaTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"-1\", \"1\"]\n",
    "max_length = 128\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegationDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        self.label_list = [\"-1\", \"1\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.label_list\n",
    "\n",
    "    @classmethod\n",
    "    def from_tsv(cls, tsv_file, tokenizer):\n",
    "        \"\"\"Creates examples for the test set.\"\"\"\n",
    "        lines = DataProcessor._read_tsv(tsv_file)\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = 'instance-%d' % i\n",
    "            if line[0] in labels:\n",
    "                text_a = '\\t'.join(line[1:])\n",
    "            else:\n",
    "                text_a = '\\t'.join(line)\n",
    "\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=None))\n",
    "\n",
    "        features = glue_convert_examples_to_features(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            max_length=max_length,\n",
    "            label_list=labels,\n",
    "            output_mode='classification',\n",
    "        )\n",
    "        return cls(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import nested_concat,nested_numpify,Any\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers.file_utils import is_torch_tpu_available\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "from typing import NamedTuple, Union,Tuple,Optional,Dict\n",
    "import numpy as np\n",
    "import logging\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "class sfdaPredictionOutput(NamedTuple):\n",
    "    predictions: Union[np.ndarray, Tuple[np.ndarray]]\n",
    "    label_ids: Optional[np.ndarray]\n",
    "    metrics: Optional[Dict[str, float]]\n",
    "    feat_matrix: Optional[np.ndarray]\n",
    "\n",
    "class sfdaTrainer(Trainer):\n",
    "        def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "    ):\n",
    "            super(sfdaTrainer,self).__init__(**kwargs)\n",
    "        def predict(self, test_dataset: Dataset, ret_feats: Optional[bool] = None) -> sfdaPredictionOutput:\n",
    "            \"\"\"\n",
    "            Run prediction and returns predictions and potential metrics.\n",
    "\n",
    "            Depending on the dataset and your use case, your test dataset may contain labels.\n",
    "            In that case, this method will also return metrics, like in :obj:`evaluate()`.\n",
    "\n",
    "            Args:\n",
    "                test_dataset (:obj:`Dataset`):\n",
    "                    Dataset to run the predictions on. If it is an :obj:`datasets.Dataset`, columns not accepted by the\n",
    "                    ``model.forward()`` method are automatically removed.\n",
    "\n",
    "            Returns:\n",
    "                `NamedTuple`:\n",
    "                predictions (:obj:`np.ndarray`):\n",
    "                    The predictions on :obj:`test_dataset`.\n",
    "                label_ids (:obj:`np.ndarray`, `optional`):\n",
    "                    The labels (if the dataset contained some).\n",
    "                metrics (:obj:`Dict[str, float]`, `optional`):\n",
    "                    The potential dictionary of metrics (if the dataset contained labels).\n",
    "            \"\"\"\n",
    "            test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "\n",
    "            return self.prediction_loop(test_dataloader, description=\"Prediction\",ret_feats = ret_feats)\n",
    "\n",
    "\n",
    "        def prediction_loop(\n",
    "        self, dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool] = None , ret_feats: Optional[bool] = None,\n",
    "        ) -> sfdaPredictionOutput:\n",
    "            \"\"\"\n",
    "            Prediction/evaluation loop, shared by :obj:`Trainer.evaluate()` and :obj:`Trainer.predict()`.\n",
    "\n",
    "            Works both with or without labels.\n",
    "            \"\"\"\n",
    "            if hasattr(self, \"_prediction_loop\"):\n",
    "                warnings.warn(\n",
    "                    \"The `_prediction_loop` method is deprecated and won't be called in a future version, define `prediction_loop` in your subclass.\",\n",
    "                    FutureWarning,\n",
    "                )\n",
    "                return self._prediction_loop(dataloader, description, prediction_loss_only=prediction_loss_only)\n",
    "\n",
    "            prediction_loss_only = (\n",
    "                prediction_loss_only if prediction_loss_only is not None else self.args.prediction_loss_only\n",
    "            )\n",
    "\n",
    "            assert not getattr(\n",
    "                self.model.config, \"output_attentions\", False\n",
    "            ), \"The prediction loop does not work with `output_attentions=True`.\"\n",
    "            assert not getattr(\n",
    "                self.model.config, \"output_hidden_states\", False\n",
    "            ), \"The prediction loop does not work with `output_hidden_states=True`.\"\n",
    "\n",
    "            model = self.model\n",
    "            # multi-gpu eval\n",
    "            if self.args.n_gpu > 1:\n",
    "                model = torch.nn.DataParallel(model)\n",
    "            else:\n",
    "                model = self.model\n",
    "            # Note: in torch.distributed mode, there's no point in wrapping the model\n",
    "            # inside a DistributedDataParallel as we'll be under `no_grad` anyways.\n",
    "\n",
    "            batch_size = dataloader.batch_size\n",
    "            logger.info(\"***** Running %s *****\", description)\n",
    "            logger.info(\"  Num examples = %d\", self.num_examples(dataloader))\n",
    "            logger.info(\"  Batch size = %d\", batch_size)\n",
    "            eval_losses: List[float] = []\n",
    "            preds: torch.Tensor = None\n",
    "            label_ids: torch.Tensor = None\n",
    "            feat_mat: torch.Tensor = None\n",
    "            model.eval()\n",
    "\n",
    "            if is_torch_tpu_available():\n",
    "                dataloader = pl.ParallelLoader(dataloader, [self.args.device]).per_device_loader(self.args.device)\n",
    "\n",
    "            if self.args.past_index >= 0:\n",
    "                self._past = None\n",
    "\n",
    "            disable_tqdm = not self.is_local_process_zero() or self.args.disable_tqdm\n",
    "            for inputs in tqdm(dataloader, desc=description, disable=disable_tqdm):\n",
    "                loss, logits, labels,feats = self.prediction_step(model, inputs, prediction_loss_only,ret_feats = ret_feats)\n",
    "                batch_size = inputs[list(inputs.keys())[0]].shape[0]\n",
    "                if loss is not None:\n",
    "                    eval_losses.extend([loss] * batch_size)\n",
    "                if logits is not None:\n",
    "                    preds = logits if preds is None else nested_concat(preds, logits, dim=0)\n",
    "                if labels is not None:\n",
    "                    label_ids = labels if label_ids is None else nested_concat(label_ids, labels, dim=0)\n",
    "                if feats is not None:\n",
    "                    feat_mat = feats if feat_mat is None else nested_concat(feat_mat,feats)\n",
    "\n",
    "            if self.args.past_index and hasattr(self, \"_past\"):\n",
    "                # Clean the state at the end of the evaluation loop\n",
    "                delattr(self, \"_past\")\n",
    "\n",
    "            if self.args.local_rank != -1:\n",
    "                # In distributed mode, concatenate all results from all nodes:\n",
    "                if preds is not None:\n",
    "                    preds = distributed_concat(preds, num_total_examples=self.num_examples(dataloader))\n",
    "                if label_ids is not None:\n",
    "                    label_ids = distributed_concat(label_ids, num_total_examples=self.num_examples(dataloader))\n",
    "                if feat_mat is not None:\n",
    "                    feat_mat = distributed_concat(feat_mat, num_total_examples=self.num_examples(dataloader))\n",
    "            \n",
    "            elif is_torch_tpu_available():\n",
    "                # tpu-comment: Get all predictions and labels from all worker shards of eval dataset\n",
    "                if preds is not None:\n",
    "                    preds = nested_xla_mesh_reduce(preds, \"eval_preds\")\n",
    "                if label_ids is not None:\n",
    "                    label_ids = nested_xla_mesh_reduce(label_ids, \"eval_label_ids\")\n",
    "                if feat_mat is not None:\n",
    "                    feat_mat = nested_xla_mesh_reduce(feat_mat, \"eval_feat_mat\")\n",
    "                if eval_losses is not None:\n",
    "                    eval_losses = xm.mesh_reduce(\"eval_losses\", torch.tensor(eval_losses), torch.cat).tolist()\n",
    "\n",
    "            # Finally, turn the aggregated tensors into numpy arrays.\n",
    "            if preds is not None:\n",
    "                preds = nested_numpify(preds)\n",
    "            if label_ids is not None:\n",
    "                label_ids = nested_numpify(label_ids)\n",
    "            if feat_mat is not None:\n",
    "                feat_mat = nested_numpify(feat_mat)\n",
    "\n",
    "            if self.compute_metrics is not None and preds is not None and label_ids is not None:\n",
    "                metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n",
    "            else:\n",
    "                metrics = {}\n",
    "            if len(eval_losses) > 0:\n",
    "                if self.args.local_rank != -1:\n",
    "                    metrics[\"eval_loss\"] = (\n",
    "                        distributed_broadcast_scalars(eval_losses, num_total_examples=self.num_examples(dataloader))\n",
    "                        .mean()\n",
    "                        .item()\n",
    "                    )\n",
    "                else:\n",
    "                    metrics[\"eval_loss\"] = np.mean(eval_losses)\n",
    "\n",
    "            # Prefix all keys with eval_\n",
    "            for key in list(metrics.keys()):\n",
    "                if not key.startswith(\"eval_\"):\n",
    "                    metrics[f\"eval_{key}\"] = metrics.pop(key)\n",
    "\n",
    "            return sfdaPredictionOutput(predictions=preds, label_ids=label_ids, metrics=metrics,feat_matrix = feat_mat)\n",
    "\n",
    "\n",
    "        def prediction_step(\n",
    "        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ret_feats: bool,\n",
    "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor],Optional[torch.Tensor]]:\n",
    "            \"\"\"\n",
    "            Perform an evaluation step on :obj:`model` using obj:`inputs`.\n",
    "\n",
    "            Subclass and override to inject custom behavior.\n",
    "\n",
    "            Args:\n",
    "                model (:obj:`nn.Module`):\n",
    "                    The model to evaluate.\n",
    "                inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                    The inputs and targets of the model.\n",
    "\n",
    "                    The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                    argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "                prediction_loss_only (:obj:`bool`):\n",
    "                    Whether or not to return the loss only.\n",
    "\n",
    "            Return:\n",
    "                Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "                A tuple with the loss, logits and labels (each being optional).\n",
    "            \"\"\"\n",
    "            has_labels = all(inputs.get(k) is not None for k in self.args.label_names)\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if ret_feats is True:\n",
    "                    outputs = model(**inputs,return_dict = True)\n",
    "#                     print(outputs)\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "                    feats = outputs.last_hidden_state[:,0,:].detach()\n",
    "                    labels = None\n",
    "                    if has_labels:\n",
    "                        # The .mean() is to reduce in case of distributed training\n",
    "                        loss = loss.mean().item()\n",
    "                        labels = tuple(inputs.get(name).detach() for name in self.args.label_names)\n",
    "                        if len(labels) == 1:\n",
    "                            labels = labels[0]\n",
    "                            \n",
    "                    return (loss, logits, labels,feats)\n",
    "                else:\n",
    "                    feats = None\n",
    "                    outputs = model(**inputs)\n",
    "                    if has_labels:\n",
    "                        # The .mean() is to reduce in case of distributed training\n",
    "                        loss = outputs[0].mean().item()\n",
    "                        logits = outputs[1:]\n",
    "                    else:\n",
    "                        loss = None\n",
    "                        # Slicing so we get a tuple even if `outputs` is a `ModelOutput`.\n",
    "                        logits = outputs[:]\n",
    "                    if self.args.past_index >= 0:\n",
    "                        self._past = outputs[self.args.past_index if has_labels else self.args.past_index - 1]\n",
    "\n",
    "            if prediction_loss_only:\n",
    "                return (loss, None, None,feats)\n",
    "\n",
    "            logits = tuple(logit.detach() for logit in logits)\n",
    "            if len(logits) == 1:\n",
    "                logits = logits[0]\n",
    "\n",
    "            if has_labels:\n",
    "                labels = tuple(inputs.get(name).detach() for name in self.args.label_names)\n",
    "                if len(labels) == 1:\n",
    "                    labels = labels[0]\n",
    "            else:\n",
    "                labels = None\n",
    "#             print(logits.shape)\n",
    "            return (loss, logits, labels,feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file, output_dir = \"../../practice_text/negation/dev.tsv\", \"../../Output/negation/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tmills/roberta_sfda_sharpseed\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, config=config)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"architectures\": [\n",
       "    \"RobertaForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"finetuning_task\": \"negation\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"vocab_size\": 50267\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tmills/roberta_sfda_sharpseed were not used when initializing sfdaRobertaNegation: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing sfdaRobertaNegation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing sfdaRobertaNegation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = sfdaRobertaNegation.from_pretrained(model_name,\n",
    "                                                           config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n"
     ]
    }
   ],
   "source": [
    "# create a torch dataset from a tsv file\n",
    "test_dataset = NegationDataset.from_tsv(data_file, tokenizer)\n",
    "\n",
    "trainer = sfdaTrainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments('save_run/'),\n",
    "    compute_metrics=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prediction: 100%|██████████| 694/694 [01:39<00:00,  6.96it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction_dict = trainer.predict(test_dataset=test_dataset,ret_feats = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = prediction_dict.predictions\n",
    "predictions = np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11388139,  0.4638905 ,  0.91159725, ...,  0.7817307 ,\n",
       "        -0.42704606,  0.6504216 ],\n",
       "       [-0.11873791,  0.465232  ,  0.90721726, ...,  0.7930985 ,\n",
       "        -0.4324146 ,  0.6495148 ],\n",
       "       [-0.0127169 , -0.15409257, -1.6846372 , ..., -0.622069  ,\n",
       "         0.56058973, -0.92781883],\n",
       "       ...,\n",
       "       [-0.07401083, -0.14768088, -1.543641  , ..., -0.6272898 ,\n",
       "         0.5359691 , -0.91750956],\n",
       "       [-0.0574165 , -0.1520247 , -1.5837629 , ..., -0.62917686,\n",
       "         0.544884  , -0.9164321 ],\n",
       "       [-0.0663449 , -0.15132469, -1.568229  , ..., -0.62588316,\n",
       "         0.54011476, -0.91360766]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dict.feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_matrix = prediction_dict.feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2886, 768)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_test_file = os.path.join(output_dir, 'dev_pred.tsv')\n",
    "feat_matrix_file = os.path.join(output_dir, 'dev_scores_and_feat_mat.npy')\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "    logger.info(\"***** Test results *****\")\n",
    "    for index, item in enumerate(predictions):\n",
    "        item = test_dataset.get_labels()[item]\n",
    "#         print(\"%s\\n\" % item)\n",
    "        writer.write(\"%s\\n\" % item)\n",
    "with open(feat_matrix_file,'wb') as file:\n",
    "    np.save(file,feat_matrix)\n",
    "    np.save(file,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-7.5.1-py2.py3-none-any.whl (121 kB)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Using cached widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipywidgets) (5.0.7)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipywidgets) (5.3.4)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipywidgets) (7.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipywidgets) (5.0.4)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.1.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: jupyter-core in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.6.3)\n",
      "Requirement already satisfied: ipython-genutils in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.0.4)\n",
      "Requirement already satisfied: jupyter-client in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.7)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (49.6.0.post20200925)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: backcall in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.7.1)\n",
      "Requirement already satisfied: pickleshare in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (19.0.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: jinja2 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: nbconvert in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.6)\n",
      "Requirement already satisfied: Send2Trash in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: testpath in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: bleach in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.2.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.3)\n",
      "Requirement already satisfied: async-generator in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.0)\n",
      "Requirement already satisfied: webencodings in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: packaging in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.4)\n",
      "Requirement already satisfied: pycparser in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/priyanshu/miniconda3/envs/CS779/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Installing collected packages: widgetsnbextension, ipywidgets\n",
      "Successfully installed ipywidgets-7.5.1 widgetsnbextension-3.5.1\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
